import pickle

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from keras import backend as K
from keras.layers import Dense, Dropout
from keras.models import Sequential
from sklearn.model_selection import train_test_split

from src.Constant import final_training_set_csv, final_selected_feature_pick, model_save_json, model_save_h5


def recall(y_true, y_pred):
    """Recall metric.

    Only computes a batch-wise average of recall.

    Computes the recall, a metric for multi-label classification of
    how many relevant items are selected.
    """
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall


def precision(y_true, y_pred):
    """Precision metric.

    Only computes a batch-wise average of precision.

    Computes the precision, a metric for multi-label classification of
    how many selected items are relevant.
    """
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision


def f1(y_true, y_pred):
    precision_s = precision(y_true, y_pred)
    recall_s = recall(y_true, y_pred)
    return 2 * ((precision_s * recall_s) / (precision_s + recall_s + K.epsilon()))


def train_keras():
    print("---train_keras---")
    # 变量初始化
    nb_epoch = 300

    df = pd.read_csv(final_training_set_csv, index_col=0)

    df = df.sample(frac=1).reset_index(drop=True)

    with open(final_selected_feature_pick, 'rb') as handle:
        feature_columns = pickle.load(handle)

    # print(feature_columns)
    df_main = df[feature_columns]

    X = np.array(df_main)
    y = np.array(df['type'])

    # print(X.shape)
    # print(df[df['type'] == 1].count())

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

    print("X_train.shape:", X_train.shape)
    print(y_train.shape)
    model = Sequential()
    model.add(Dense(250, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(250, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(250, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(250, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))

    # 打印模型
    model.summary()

    # 训练与评估
    # 编译模型
    # binary_cross_entropy是二分类的交叉熵
    # RMSprop 全称 root mean square prop 算法，和动量方法一样都可以加快梯度下降速度。
    model.compile(loss='binary_crossentropy',
                  optimizer='rmsprop',
                  metrics=['acc', precision, recall, f1])
    # 迭代训练（注意这个地方要加入callbacks）
    training = model.fit(X_train, y_train, epochs=nb_epoch, verbose=0, validation_data=(X_test, y_test))

    # 评估模型性能
    score = model.evaluate(X_test, y_test)
    print("score:", score)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])
    print('Test precision:', score[2])
    print('Test recall:', score[3])
    print('Test f1:', score[4])

    model_json = model.to_json()
    with open(model_save_json, "w") as json_file:
        json_file.write(model_json)

    model.save_weights(model_save_h5)
    print("Model saved")

    # 绘图
    print(training.history.keys())
    plt.title('model loss/accuracy')

    plt.plot(training.history['acc'], "g")
    plt.xlabel('epoch')
    plt.ylabel('acc')
    plt.legend(['acc'], loc='lower right')

    plt.plot(training.history['f1'], "y")
    plt.ylabel('f1')
    plt.legend(['f1'], loc='lower right')

    plt.plot(training.history['loss'], "r")
    plt.ylabel('loss')
    plt.legend(['loss'], loc='lower right')
    plt.show()
